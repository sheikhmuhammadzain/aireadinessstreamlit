[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "streamlit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "streamlit",
        "description": "streamlit",
        "detail": "streamlit",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "matplotlib.patches",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "get_color_for_score",
        "importPath": "helper_functions",
        "description": "helper_functions",
        "isExtraImport": true,
        "detail": "helper_functions",
        "documentation": {}
    },
    {
        "label": "get_strength_comment",
        "importPath": "helper_functions",
        "description": "helper_functions",
        "isExtraImport": true,
        "detail": "helper_functions",
        "documentation": {}
    },
    {
        "label": "get_improvement_comment",
        "importPath": "helper_functions",
        "description": "helper_functions",
        "isExtraImport": true,
        "detail": "helper_functions",
        "documentation": {}
    },
    {
        "label": "get_recommendations",
        "importPath": "helper_functions",
        "description": "helper_functions",
        "isExtraImport": true,
        "detail": "helper_functions",
        "documentation": {}
    },
    {
        "label": "create_radar_chart",
        "importPath": "visualization_functions",
        "description": "visualization_functions",
        "isExtraImport": true,
        "detail": "visualization_functions",
        "documentation": {}
    },
    {
        "label": "create_gauge_chart",
        "importPath": "visualization_functions",
        "description": "visualization_functions",
        "isExtraImport": true,
        "detail": "visualization_functions",
        "documentation": {}
    },
    {
        "label": "create_bar_chart",
        "importPath": "visualization_functions",
        "description": "visualization_functions",
        "isExtraImport": true,
        "detail": "visualization_functions",
        "documentation": {}
    },
    {
        "label": "matplotlib.colors",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.colors",
        "description": "matplotlib.colors",
        "detail": "matplotlib.colors",
        "documentation": {}
    },
    {
        "label": "LinearSegmentedColormap",
        "importPath": "matplotlib.colors",
        "description": "matplotlib.colors",
        "isExtraImport": true,
        "detail": "matplotlib.colors",
        "documentation": {}
    },
    {
        "label": "LinearSegmentedColormap",
        "importPath": "matplotlib.colors",
        "description": "matplotlib.colors",
        "isExtraImport": true,
        "detail": "matplotlib.colors",
        "documentation": {}
    },
    {
        "label": "Figure",
        "importPath": "matplotlib.figure",
        "description": "matplotlib.figure",
        "isExtraImport": true,
        "detail": "matplotlib.figure",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "questionnaire",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "questionnaire = {\n    \"AI Roles & Responsibilities\": [\n        \"Do you have policies for ethical AI development?\",\n        \"Are all stakeholders educated on AI governance policies?\",\n        \"Do you have regular AI governance board meetings?\",\n        \"Is algorithmic accountability explicitly assigned?\",\n    ],\n    \"Regulatory Compliance\": [\n        \"Does your AI system comply with GDPR, EU AI Act, ISO 42001, or other laws?\",\n        \"Are your AI policies updated with changing regulations?\",",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "user_responses",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "user_responses = {category: [] for category in questionnaire.keys()}\n# ‚úÖ Asking Questions\nfor category, questions in questionnaire.items():\n    print(f\"\\nüîπ **Category: {category}**\")\n    for question in questions:\n        while True:\n            try:\n                response = int(\n                    input(f\"{question}\\nEnter your response (1-4, where 1 = Strongly Disagree, 4 = Strongly Agree): \"))\n                if response < 1 or response > 4:",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "category_scores",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "category_scores = {category: np.mean(scores) for category, scores in user_responses.items()}\n# ‚úÖ Initialize Q-values (Reinforcement Learning)\nq_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "q_values",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "q_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "alpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "gamma",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "gamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "reward",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "reward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "eta",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "eta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Governance Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "exp_q_values",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "exp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Governance Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "softmax_weights",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "softmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Governance Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "overall_score",
        "kind": 5,
        "importPath": "ai-governance-scoring-with-qlearning-questionnaire",
        "description": "ai-governance-scoring-with-qlearning-questionnaire",
        "peekOfCode": "overall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Governance Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():\n    print(f\"{category}: {q_val:.3f}\")\n# ‚úÖ Display Softmax Weights",
        "detail": "ai-governance-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "matplotlib_to_image",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def matplotlib_to_image(fig):\n    buf = BytesIO()\n    fig.savefig(buf, format=\"png\", dpi=300, bbox_inches='tight', transparent=True)\n    buf.seek(0)\n    img_str = base64.b64encode(buf.read()).decode()\n    return f\"data:image/png;base64,{img_str}\"\n# Function to create a premium-looking radar chart with matplotlib\ndef create_radar_chart(categories, values, title):\n    # Convert values to 0-100 scale\n    values_100 = [v * 100 / 4 for v in values]",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "create_radar_chart",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def create_radar_chart(categories, values, title):\n    # Convert values to 0-100 scale\n    values_100 = [v * 100 / 4 for v in values]\n    # Number of categories\n    N = len(categories)\n    # Create angles for each category (in radians)\n    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n    angles += angles[:1]  # Close the loop\n    # Values also need to close the loop\n    values_100_closed = values_100 + values_100[:1]",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "create_category_bar_chart",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def create_category_bar_chart(categories, scores, title):\n    # Set seaborn style\n    sns.set_style(\"whitegrid\")\n    # Create a color gradient\n    colors = sns.color_palette(\"Blues_r\", len(categories))\n    # Create figure\n    fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')\n    # Create horizontal bar chart\n    bars = ax.barh(categories, [score * 100 / 4 for score in scores], color=colors, height=0.6)\n    # Add data labels",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "create_qvalue_weight_heatmap",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def create_qvalue_weight_heatmap(categories, q_values, weights, title):\n    # Prepare data for heatmap\n    data = []\n    for category, q_val, weight in zip(categories, q_values, weights):\n        data.append({\n            'Category': category,\n            'Q-Value': q_val,\n            'Weight (%)': weight * 100\n        })\n    df = pd.DataFrame(data)",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "create_gauge_chart",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def create_gauge_chart(score, title):\n    # Configure the figure\n    fig = plt.figure(figsize=(10, 6), facecolor='white')\n    ax = fig.add_subplot(111)\n    # Hide axes\n    ax.set_axis_off()\n    # Set score (0-100 scale)\n    score_100 = score * 100 / 4\n    # Define gauge colors and ranges\n    cmap = LinearSegmentedColormap.from_list('gauge_colors', ['#FF5252', '#FFAB40', '#FFEE58', '#66BB6A'])",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "extract_questionnaire_data",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def extract_questionnaire_data(file_path):\n    # Initial questionnaire structure\n    questionnaire = {}\n    try:\n        # Try with utf-8 encoding first\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n    except UnicodeDecodeError:\n        # Fall back to latin-1 if utf-8 fails\n        with open(file_path, 'r', encoding='latin-1') as file:",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "load_all_questionnaires",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def load_all_questionnaires():\n    all_questionnaires = {}\n    for category, file_name in questionnaire_files.items():\n        file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), file_name)\n        questionnaire = extract_questionnaire_data(file_path)\n        if questionnaire:\n            all_questionnaires[category] = questionnaire\n    # If no questionnaires were loaded, use fallback\n    if not all_questionnaires:\n        st.warning(\"Could not load questionnaires from files. Using embedded questionnaires.\")",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "calculate_scores",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def calculate_scores(responses):\n    category_scores = {}\n    q_values = {}\n    softmax_weights = {}\n    overall_scores = {}\n    # For each assessment category (Governance, Culture, etc.)\n    for assessment_category, assessment_data in responses.items():\n        # Initialize category scores for this assessment\n        category_scores[assessment_category] = {}\n        # Calculate mean score for each question category",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "get_color_for_score",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def get_color_for_score(score):\n    \"\"\"Return a color based on the score value.\"\"\"\n    if score < 30:\n        return \"#EF4444\"  # Red for low scores\n    elif score < 60:\n        return \"#F59E0B\"  # Amber for medium scores\n    elif score < 80:\n        return \"#10B981\"  # Green for good scores\n    else:\n        return \"#0284C7\"  # Blue for excellent scores",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "create_radar_chart",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def create_radar_chart(categories, scores):\n    \"\"\"Create a radar chart for the category scores.\"\"\"\n    # Number of variables\n    N = len(categories)\n    # What will be the angle of each axis in the plot\n    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n    angles += angles[:1]  # Close the loop\n    # Scores need to be in the same order and length as angles\n    scores_for_plot = scores.copy()\n    scores_for_plot += scores_for_plot[:1]  # Close the loop",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "create_bar_chart",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def create_bar_chart(categories, scores):\n    \"\"\"Create a horizontal bar chart for category scores.\"\"\"\n    # Format categories for display\n    display_categories = [cat.replace('AI ', '') for cat in categories]\n    # Create the bar chart\n    fig, ax = plt.figure(figsize=(10, 6)), plt.axes()\n    # Plot horizontal bars\n    bars = ax.barh(display_categories, scores, color='#0284C7', alpha=0.7, height=0.5)\n    # Add value labels to the right of each bar\n    for bar in bars:",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "create_gauge_chart",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def create_gauge_chart(score):\n    \"\"\"Create a gauge chart for the overall score.\"\"\"\n    # Define the score ranges and colors\n    ranges = [0, 30, 60, 80, 100]\n    colors = ['#EF4444', '#F59E0B', '#10B981', '#0284C7']\n    # Create the figure\n    fig, ax = plt.subplots(figsize=(4, 4), subplot_kw={'projection': 'polar'})\n    # Set the gauge limits (in radians)\n    start_angle = 3*np.pi/4\n    end_angle = -np.pi/4",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "get_strength_comment",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def get_strength_comment(category, score):\n    \"\"\"Return a comment about the organization's strength in a specific category.\"\"\"\n    if \"Data\" in category:\n        return \"Strong data management practices and governance provide a solid foundation for AI initiatives.\"\n    elif \"Infrastructure\" in category:\n        return \"Robust technical infrastructure and computing resources enable efficient AI model training and deployment.\"\n    elif \"Talent\" in category:\n        return \"Well-developed AI talent acquisition, training, and retention strategies support AI capabilities.\"\n    elif \"Strategy\" in category:\n        return \"Clear AI strategy aligned with business objectives provides direction for AI initiatives.\"",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "get_improvement_comment",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def get_improvement_comment(category, score):\n    \"\"\"Return a comment about areas for improvement in a specific category.\"\"\"\n    if \"Data\" in category:\n        return \"Enhance data quality, accessibility, governance, and management practices to build a stronger foundation for AI.\"\n    elif \"Infrastructure\" in category:\n        return \"Invest in technical infrastructure, cloud resources, and MLOps capabilities to support AI initiatives.\"\n    elif \"Talent\" in category:\n        return \"Develop structured talent acquisition, upskilling programs, and retention strategies for AI professionals.\"\n    elif \"Strategy\" in category:\n        return \"Create a more comprehensive AI strategy aligned with business objectives and develop clear roadmaps.\"",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "get_recommendations",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def get_recommendations(category, score):\n    \"\"\"Return specific recommendations based on category and score.\"\"\"\n    recommendations = []\n    if \"Data\" in category:\n        recommendations = [\n            \"Implement a comprehensive data governance framework with clear ownership and quality standards\",\n            \"Develop a centralized data catalog to improve accessibility and discoverability\",\n            \"Establish data quality monitoring processes specific to AI use cases\",\n            \"Create standardized data preparation pipelines for common AI scenarios\"\n        ]",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def main():\n    # Setup session state for storing responses if not already present\n    if 'responses' not in st.session_state:\n        st.session_state.responses = {}\n    if 'assessment_started' not in st.session_state:\n        st.session_state.assessment_started = False\n    if 'show_results' not in st.session_state:\n        st.session_state.show_results = False\n    # Initialize navigation if not already set\n    if 'nav' not in st.session_state:",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "show_home_page",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def show_home_page():\n    # Header with logo\n    st.markdown(\"\"\"\n    <div class=\"dashboard-header\">\n        <h1 class=\"dashboard-title\">AI Readiness Assessment</h1>\n        <p class=\"dashboard-subtitle\">Evaluate your organization's capacity to implement and scale AI solutions</p>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n    # Overview section\n    st.markdown(\"\"\"",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "show_about_page",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def show_about_page():\n    st.markdown(\"\"\"\n    <div class=\"card\">\n        <h2>About the AI Readiness Assessment Tool</h2>\n        <p>\n            This enterprise-grade assessment tool helps organizations evaluate their AI readiness \n            across six critical dimensions using a sophisticated Q-Learning algorithm.\n        </p>\n        <h3>Methodology</h3>\n        <p>",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "show_questionnaire",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def show_questionnaire(all_questionnaires):\n    st.header(\"AI Readiness Questionnaire\")\n    # Check if we have any questionnaires to display\n    if not all_questionnaires:\n        st.warning(\"No questionnaires were loaded successfully. Please check your installation.\")\n        return\n    st.markdown(\"\"\"\n    <div class=\"dashboard-header\">\n        <h2 class=\"dashboard-title\">AI Readiness Questionnaire</h2>\n        <p class=\"dashboard-subtitle\">Evaluate your organization's AI readiness across key dimensions</p>",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "show_results",
        "kind": 2,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "def show_results():\n    # Calculate the overall score and category scores\n    overall_score = 0\n    category_scores = {}\n    for category, category_responses in st.session_state.responses.items():\n        category_score = 0\n        question_count = 0\n        for q_category, responses in category_responses.items():\n            for response in responses:\n                if response is not None:",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "theme_colors",
        "kind": 5,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "theme_colors = {\n    'primary': '#1E293B',     # Slate 800\n    'secondary': '#475569',   # Slate 600\n    'accent': '#0284C7',      # Sky 600\n    'success': '#059669',     # Emerald 600\n    'warning': '#D97706',     # Amber 600\n    'error': '#EF4444',       # Red 600\n    'background': '#F8FAFC',  # Slate 50\n    'card': '#FFFFFF',        # White\n    'text': '#334155',        # Slate 700",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "questionnaire_files",
        "kind": 5,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "questionnaire_files = {\n    \"AI Governance\": \"ai-governance-scoring-with-qlearning-questionnaire.py\",\n    \"AI Culture\": \"culture-scoring-with-qlearning-questionnaire.py\",\n    \"AI Data\": \"data-scoring-with-qlearning-questionnaire.py\",\n    \"AI Infrastructure\": \"infra-scoring-with-qlearning-questionnaire.py\",\n    \"AI Strategy\": \"strategy-scoring-with-qlearning-questionnaire.py\",\n    \"AI Talent\": \"talen-scoring-with-qlearning-questionnaire.py\"\n}\n# Fallback questionnaires in case file parsing fails\nfallback_questionnaires = {",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "fallback_questionnaires",
        "kind": 5,
        "importPath": "ai_readiness_assessment_app",
        "description": "ai_readiness_assessment_app",
        "peekOfCode": "fallback_questionnaires = {\n    \"AI Governance\": {\n        \"AI Roles & Responsibilities\": [\n            \"Do you have policies for ethical AI development?\",\n            \"Are all stakeholders educated on AI governance policies?\",\n            \"Do you have regular AI governance board meetings?\",\n            \"Is algorithmic accountability explicitly assigned?\"\n        ],\n        \"Regulatory Compliance\": [\n            \"Does your AI system comply with GDPR, EU AI Act, ISO 42001, or other laws?\",",
        "detail": "ai_readiness_assessment_app",
        "documentation": {}
    },
    {
        "label": "questionnaire",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "questionnaire = {\n    \"AI Leadership & Vision\": [\n        \"Does the organization have a clear AI strategy?\",\n        \"Is there an AI champions program in your organization?\",\n        \"Do you integrate AI into day-to-day organizational workflows?\"\n    ],\n    \"AI Experimentation & Innovation\": [\n        \"Are AI pilots and innovation hubs encouraged?\",\n        \"Do you celebrate AI project milestones publicly?\",\n        \"Do you have a platform for sharing AI-related innovations?\",",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "user_responses",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "user_responses = {category: [] for category in questionnaire.keys()}\n# ‚úÖ Asking Questions\nfor category, questions in questionnaire.items():\n    print(f\"\\nüîπ **Category: {category}**\")\n    for question in questions:\n        while True:\n            try:\n                response = int(\n                    input(f\"{question}\\nEnter your response (1-4, where 1 = Strongly Disagree, 4 = Strongly Agree): \"))\n                if response < 1 or response > 4:",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "category_scores",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "category_scores = {category: np.mean(scores) for category, scores in user_responses.items()}\n# ‚úÖ Initialize Q-values (Reinforcement Learning)\nq_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "q_values",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "q_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "alpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "gamma",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "gamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "reward",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "reward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "eta",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "eta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Culture Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "exp_q_values",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "exp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Culture Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "softmax_weights",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "softmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Culture Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "overall_score",
        "kind": 5,
        "importPath": "culture-scoring-with-qlearning-questionnaire",
        "description": "culture-scoring-with-qlearning-questionnaire",
        "peekOfCode": "overall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Culture Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():\n    print(f\"{category}: {q_val:.3f}\")\n# ‚úÖ Display Softmax Weights",
        "detail": "culture-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "questionnaire",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "questionnaire = {\n    \"Data Accessibility & Cataloging\": [\n        \"Is your data catalogued for easy access by AI teams?\",\n        \"Do you maintain data dictionaries to standardize metadata?\",\n        \"How frequent is your data updated for AI use cases?\",\n        \"Do you support multi-modal data integration for AI?\",\n    ],\n    \"Data Governance & Compliance\": [\n        \"Is your data governance aligned with AI requirements?\",\n        \"How secure is your external data ingestion process?\",",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "user_responses",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "user_responses = {category: [] for category in questionnaire.keys()}\n# ‚úÖ Asking Questions\nfor category, questions in questionnaire.items():\n    print(f\"\\nüîπ **Category: {category}**\")\n    for question in questions:\n        while True:\n            try:\n                response = int(\n                    input(f\"{question}\\nEnter your response (1-4, where 1 = Strongly Disagree, 4 = Strongly Agree): \"))\n                if response < 1 or response > 4:",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "category_scores",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "category_scores = {category: np.mean(scores) for category, scores in user_responses.items()}\n# ‚úÖ Initialize Q-values (Reinforcement Learning)\nq_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "q_values",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "q_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "alpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "gamma",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "gamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "reward",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "reward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "eta",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "eta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Data Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "exp_q_values",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "exp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Data Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "softmax_weights",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "softmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Data Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "overall_score",
        "kind": 5,
        "importPath": "data-scoring-with-qlearning-questionnaire",
        "description": "data-scoring-with-qlearning-questionnaire",
        "peekOfCode": "overall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Data Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():\n    print(f\"{category}: {q_val:.3f}\")\n# ‚úÖ Display Softmax Weights",
        "detail": "data-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "get_color_for_score",
        "kind": 2,
        "importPath": "helper_functions",
        "description": "helper_functions",
        "peekOfCode": "def get_color_for_score(score):\n    \"\"\"\n    Returns an appropriate color based on the score range\n    \"\"\"\n    if score < 30:\n        return \"#EF4444\"  # Red\n    elif score < 60:\n        return \"#F59E0B\"  # Amber\n    elif score < 80:\n        return \"#10B981\"  # Green",
        "detail": "helper_functions",
        "documentation": {}
    },
    {
        "label": "get_strength_comment",
        "kind": 2,
        "importPath": "helper_functions",
        "description": "helper_functions",
        "peekOfCode": "def get_strength_comment(category, score):\n    \"\"\"\n    Return a customized strength comment based on the category and score\n    \"\"\"\n    if category == \"AI Governance\":\n        return \"Your organization has established strong AI governance processes with clear policies and accountability structures.\"\n    elif category == \"AI Data\":\n        return \"You have robust data management practices, with high-quality data that is well organized and accessible.\"\n    elif category == \"AI Infrastructure\":\n        return \"Your technical infrastructure is well-equipped to support AI initiatives with adequate compute and scalable environments.\"",
        "detail": "helper_functions",
        "documentation": {}
    },
    {
        "label": "get_improvement_comment",
        "kind": 2,
        "importPath": "helper_functions",
        "description": "helper_functions",
        "peekOfCode": "def get_improvement_comment(category, score):\n    \"\"\"\n    Return a customized improvement comment based on the category and score\n    \"\"\"\n    if category == \"AI Governance\":\n        return \"Focus on developing structured AI governance policies and clearer accountability frameworks.\"\n    elif category == \"AI Data\":\n        return \"Improve data quality, accessibility, and implement better data governance practices.\"\n    elif category == \"AI Infrastructure\":\n        return \"Invest in more robust AI infrastructure and MLOps capabilities to support AI initiatives.\"",
        "detail": "helper_functions",
        "documentation": {}
    },
    {
        "label": "get_recommendations",
        "kind": 2,
        "importPath": "helper_functions",
        "description": "helper_functions",
        "peekOfCode": "def get_recommendations(category, score):\n    \"\"\"\n    Return a list of customized recommendations based on the category and score\n    \"\"\"\n    recommendations = []\n    if category == \"AI Governance\":\n        recommendations = [\n            \"Establish clear policies for ethical AI development and usage\",\n            \"Create explicit roles for algorithmic accountability and oversight\",\n            \"Develop a comprehensive AI risk management framework\",",
        "detail": "helper_functions",
        "documentation": {}
    },
    {
        "label": "questionnaire",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "questionnaire = {\n    \"Compute Resources\": [\n        \"Do you have the necessary GPUs/TPUs for AI workloads?\",\n        \"Are your compute resources sufficient for AI training tasks?\",\n        \"Do you have redundancy systems for AI computing needs?\",\n        \"How aligned are your hardware upgrades with AI goals?\",\n        \"Do you have a separate budget for scaling AI infrastructure?\"\n    ],\n    \"Storage & Data Access\": [\n        \"Is your data storage optimized for AI scalability?\",",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "user_responses",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "user_responses = {category: [] for category in questionnaire.keys()}\n# ‚úÖ Asking Questions\nfor category, questions in questionnaire.items():\n    print(f\"\\nüîπ **Category: {category}**\")\n    for question in questions:\n        while True:\n            try:\n                response = int(\n                    input(f\"{question}\\nEnter your response (1-4, where 1 = Strongly Disagree, 4 = Strongly Agree): \"))\n                if response < 1 or response > 4:",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "category_scores",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "category_scores = {category: np.mean(scores) for category, scores in user_responses.items()}\n# ‚úÖ Initialize Q-values (Reinforcement Learning)\nq_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "q_values",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "q_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "alpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "gamma",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "gamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "reward",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "reward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "eta",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "eta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Infrastructure Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "exp_q_values",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "exp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Infrastructure Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "softmax_weights",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "softmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Infrastructure Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "overall_score",
        "kind": 5,
        "importPath": "infra-scoring-with-qlearning-questionnaire",
        "description": "infra-scoring-with-qlearning-questionnaire",
        "peekOfCode": "overall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Infrastructure Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():\n    print(f\"{category}: {q_val:.3f}\")\n# ‚úÖ Display Softmax Weights",
        "detail": "infra-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "questionnaire",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "questionnaire = {\n    \"Data Security & Encryption\": [\n        \"Is training and inference data encrypted and secured?\",\n        \"How robust are your organization's data encryption and protection mechanisms for AI deployments?\",\n        \"Does your organization conduct regular security audits for AI systems?\",\n        \"How does your organization ensure compliance with regulations regarding AI security?\"\n    ],\n    \"Model Access Control\": [\n        \"Do you employ explainable AI techniques to enhance transparency?\",\n        \"Are access controls for AI systems well-defined and enforced?\",",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "user_responses",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "user_responses = {category: [] for category in questionnaire.keys()}\n# ‚úÖ Asking Questions\nfor category, questions in questionnaire.items():\n    print(f\"\\nüîπ **Category: {category}**\")\n    for question in questions:\n        while True:\n            try:\n                response = int(\n                    input(f\"{question}\\nEnter your response (1-4, where 1 = Strongly Disagree, 4 = Strongly Agree): \"))\n                if response < 1 or response > 4:",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "category_scores",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "category_scores = {category: np.mean(scores) for category, scores in user_responses.items()}\n# ‚úÖ Initialize Q-values (Reinforcement Learning)\nq_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "q_values",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "q_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "alpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "gamma",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "gamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "reward",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "reward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "eta",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "eta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Strategy Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "exp_q_values",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "exp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Strategy Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "softmax_weights",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "softmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Strategy Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "overall_score",
        "kind": 5,
        "importPath": "strategy-scoring-with-qlearning-questionnaire",
        "description": "strategy-scoring-with-qlearning-questionnaire",
        "peekOfCode": "overall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Strategy Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():\n    print(f\"{category}: {q_val:.3f}\")\n# ‚úÖ Display Softmax Weights",
        "detail": "strategy-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "questionnaire",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "questionnaire = {\n    \"AI Talent Acquisition\": [\n        \"Do you have a structured AI hiring strategy?\",\n        \"Are your hiring policies tailored for AI-specific skills?\",\n        \"Do you recruit globally for AI talent?\",\n        \"Are your job roles for AI well-defined?\",\n    ],\n    \"AI Upskilling & Training\": [\n        \"Are employees receiving AI training and certifications?\",\n        \"Do you conduct regular AI capability assessments for your staff?\",",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "user_responses",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "user_responses = {category: [] for category in questionnaire.keys()}\n# ‚úÖ Asking Questions\nfor category, questions in questionnaire.items():\n    print(f\"\\nüîπ **Category: {category}**\")\n    for question in questions:\n        while True:\n            try:\n                response = int(\n                    input(f\"{question}\\nEnter your response (1-4, where 1 = Strongly Disagree, 4 = Strongly Agree): \"))\n                if response < 1 or response > 4:",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "category_scores",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "category_scores = {category: np.mean(scores) for category, scores in user_responses.items()}\n# ‚úÖ Initialize Q-values (Reinforcement Learning)\nq_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "q_values",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "q_values = {category: np.random.uniform(0, 1) for category in questionnaire.keys()}\n# ‚úÖ Reinforcement Learning Parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "alpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "gamma",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "gamma = 0.9  # Discount factor\nreward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "reward",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "reward = 1  # Assume a reward of 1 for simplicity\n# ‚úÖ Updating Q-values iteratively based on reinforcement learning\nfor _ in range(10):  # Simulate multiple learning iterations\n    for category in questionnaire.keys():\n        q_values[category] = q_values[category] + alpha * (reward + gamma * max(q_values.values()) - q_values[category])\n# ‚úÖ Compute Softmax Weights **AFTER UPDATING Q-values**\neta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "eta",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "eta = 1.0  # Softmax scaling parameter\nexp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Talent Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "exp_q_values",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "exp_q_values = np.exp(eta * np.array(list(q_values.values())))\nsoftmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Talent Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "softmax_weights",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "softmax_weights = exp_q_values / np.sum(exp_q_values)\n# ‚úÖ Compute Final AI Data Readiness Score (Weighted Sum)\noverall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Talent Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "overall_score",
        "kind": 5,
        "importPath": "talen-scoring-with-qlearning-questionnaire",
        "description": "talen-scoring-with-qlearning-questionnaire",
        "peekOfCode": "overall_score = sum(category_scores[cat] * softmax_weights[i] for i, cat in enumerate(questionnaire.keys()))\n# ‚úÖ Display Results\nprint(\"\\nüèÜ **AI Talent Readiness Scores:**\")\nfor category in questionnaire.keys():\n    print(f\"{category}: {category_scores[category]:.2f}\")\n# ‚úÖ Display Updated Q-values\nprint(\"\\nüîπ **Updated Q-values after Learning:**\")\nfor category, q_val in q_values.items():\n    print(f\"{category}: {q_val:.3f}\")\n# ‚úÖ Display Softmax Weights",
        "detail": "talen-scoring-with-qlearning-questionnaire",
        "documentation": {}
    },
    {
        "label": "create_radar_chart",
        "kind": 2,
        "importPath": "visualization_functions",
        "description": "visualization_functions",
        "peekOfCode": "def create_radar_chart(categories, values):\n    \"\"\"\n    Create a radar chart for displaying category scores\n    \"\"\"\n    # Convert values to numpy array\n    values = np.array(values)\n    # Number of variables\n    N = len(categories)\n    # What will be the angle of each axis in the plot\n    angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()",
        "detail": "visualization_functions",
        "documentation": {}
    },
    {
        "label": "create_gauge_chart",
        "kind": 2,
        "importPath": "visualization_functions",
        "description": "visualization_functions",
        "peekOfCode": "def create_gauge_chart(score):\n    \"\"\"\n    Create a gauge chart for displaying the overall score\n    \"\"\"\n    # Start and end angle for the gauge\n    start_angle = np.pi/2 + np.pi/4\n    end_angle = np.pi/2 - np.pi/4\n    # Create figure and axis\n    fig, ax = plt.subplots(figsize=(6, 4), subplot_kw=dict(polar=True))\n    # Define ranges for different color segments",
        "detail": "visualization_functions",
        "documentation": {}
    },
    {
        "label": "create_bar_chart",
        "kind": 2,
        "importPath": "visualization_functions",
        "description": "visualization_functions",
        "peekOfCode": "def create_bar_chart(categories, values):\n    \"\"\"\n    Create a bar chart for displaying category scores\n    \"\"\"\n    # Create figure\n    fig, ax = plt.subplots(figsize=(10, 6))\n    # Format categories for display (remove 'AI ' prefix if present)\n    display_categories = [cat.replace('AI ', '') if 'AI ' in cat else cat for cat in categories]\n    # Create horizontal bar chart with colors based on values\n    colors = [theme_colors[\"primary\"] if v >= 60 else theme_colors[\"warning\"] if v >= 30 else theme_colors[\"error\"] for v in values]",
        "detail": "visualization_functions",
        "documentation": {}
    },
    {
        "label": "theme_colors",
        "kind": 5,
        "importPath": "visualization_functions",
        "description": "visualization_functions",
        "peekOfCode": "theme_colors = {\n    \"primary\": \"#0284C7\",\n    \"secondary\": \"#38BDF8\",\n    \"success\": \"#10B981\",\n    \"warning\": \"#F59E0B\",\n    \"error\": \"#EF4444\",\n    \"text\": \"#1E293B\",\n    \"background\": \"#F1F5F9\"\n}\ndef create_radar_chart(categories, values):",
        "detail": "visualization_functions",
        "documentation": {}
    }
]